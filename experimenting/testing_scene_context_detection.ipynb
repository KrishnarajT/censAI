{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load BLIP model for image captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    \"\"\"Generate a caption for the given image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "\n",
    "    caption = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "    return caption\n",
    "\n",
    "\n",
    "def is_explicit_scene(image_path):\n",
    "    caption = generate_caption(image_path)\n",
    "    explicit_keywords = [\n",
    "        \"bedroom\",\n",
    "        \"kissing\",\n",
    "        \"naked\",\n",
    "        \"intimate\",\n",
    "        \"erotic\",\n",
    "        \"romantic\",\n",
    "        \"undressed\",\n",
    "    ]\n",
    "\n",
    "    for word in explicit_keywords:\n",
    "        if word in caption.lower():\n",
    "            return True  # Scene flagged as explicit\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a naked girl sitting on the rocks by the ocean\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "caption = generate_caption(\"images/8.jpg\")\n",
    "print(\"Caption:\", caption)\n",
    "print(is_explicit_scene(\"images/8.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a woman is laying down on a bed\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "caption = generate_caption(\"images/7.jpg\")\n",
    "print(\"Caption:\", caption)\n",
    "print(is_explicit_scene(\"images/7.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scene.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mis_explicit_scene\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscene.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# True if explicit\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mis_explicit_scene\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_explicit_scene\u001b[39m(image_path):\n\u001b[1;32m---> 25\u001b[0m     caption \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     explicit_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedroom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkissing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mundressed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     ]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m explicit_keywords:\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mgenerate_caption\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_caption\u001b[39m(image_path):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a caption for the given image.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scene.jpg'"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "print(is_explicit_scene(\"scene.jpg\"))  # True if explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
    ")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\n",
    "    \"nlpconnect/vit-gpt2-image-captioning\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a man standing next to a statue of a cow']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(\n",
    "    [\"images/5.jpg\"]\n",
    ")  # ['a woman in a hospital bed with a woman in a hospital bed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'a woman is laying down on a bed '}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "image_to_text(\"images/7.jpg\")\n",
    "\n",
    "# [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "import torch\n",
    "model_id = \"microsoft/Florence-2-large\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")  # Move model to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"images/7.jpg\"  # Update this with your local image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Process the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: woman\n"
     ]
    }
   ],
   "source": [
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "# Decode and print caption\n",
    "caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Caption: The image shows a close-up of a woman lying on her back on a bed.\n"
     ]
    }
   ],
   "source": [
    "task_prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "inputs = processor(images=image, text=task_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(\"Detailed Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_caption(image_path, task_prompt=\"<MORE_DETAILED_CAPTION>\"):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Tokenize & Encode\n",
    "    inputs = processor(text=task_prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # ðŸ”¥ Move **ALL** tensors to CUDA\n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "\n",
    "    # Generate Caption with Beam Search\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,  # Allow longer captions\n",
    "            early_stopping=False,\n",
    "            do_sample=False,  # No randomness\n",
    "            num_beams=3,  # Beam search for better results\n",
    "        )\n",
    "\n",
    "    # Decode Output\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Post-process (Ensure Processing Happens on CUDA)\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task=task_prompt,\n",
    "        image_size=image.size\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "def generate_detailed_caption_exp(image_path, task_prompt=\"<MORE_DETAILED_CAPTION>\"):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Tokenize & Encode\n",
    "    inputs = processor(text=task_prompt, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # ðŸ”¥ Move **ALL** tensors to CUDA\n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "\n",
    "    # Generate Caption with Beam Search\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=2048,  # Allow longer captions\n",
    "            early_stopping=False,\n",
    "            do_sample=True,  # No randomness\n",
    "            num_beams=3,  # Beam search for better results\n",
    "        )\n",
    "\n",
    "    # Decode Output\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Post-process (Ensure Processing Happens on CUDA)\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task=task_prompt,\n",
    "        image_size=image.size\n",
    "    )\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/7.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image file path\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m detailed_caption \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_detailed_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m exp_detailed_caption \u001b[38;5;241m=\u001b[39m generate_detailed_caption_exp(image_path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard base caption\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mgenerate_detailed_caption\u001b[1;34m(image_path, task_prompt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_detailed_caption\u001b[39m(image_path, task_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<MORE_DETAILED_CAPTION>\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize & Encode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtask_prompt, images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = \"images/7.jpg\"  # Replace with your image file path\n",
    "detailed_caption = generate_detailed_caption(image_path)\n",
    "exp_detailed_caption = generate_detailed_caption_exp(image_path)\n",
    "print(\"standard base caption\")\n",
    "print(detailed_caption)\n",
    "print(\"experimental detailed caption\")\n",
    "print(exp_detailed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/1.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image shows two women in a room with a bed in the background. The woman on the left is naked and has long red hair that is styled in two braids. She is looking at the other woman with a smile on her face. The other woman is standing in front of her and appears to be looking at her with a concerned expression. She has blonde hair that falls over her shoulders and is wearing a green blouse. The room is cluttered with clothes and other items, and there is a window on the right side of the image.'}\n",
      "images/2.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image shows a scene from the TV show Game of Thrones. It shows a young man and a young woman sitting on a bed, facing each other. The man is holding a baby in his arms and the woman is looking at him with a concerned expression. The bed is covered with a black blanket and there is a fireplace in the background with lit candles on it. The room appears to be dimly lit and there are curtains on the windows. The text on the image reads \"And the other brother?\"'}\n",
      "images/3.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': \"The image shows a close-up of a man and a woman's faces. The woman is on the left side of the image, facing away from the camera, with her head tilted slightly to the side. She has shoulder-length dark hair and appears to be in deep thought. The man on the right side is facing the woman, with a serious expression on his face. The background is blurred, but it seems to be a dimly lit room with a window and a plant visible. The image is likely from a scene from a movie or TV show.\"}\n",
      "images/4.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image shows a young man with blonde hair, holding a black pitcher in his right hand. He appears to be in a dimly lit room with a stone wall in the background. The man is wearing a green shirt and has a serious expression on his face. There are several lit candles scattered around the room, creating a cozy atmosphere. The overall mood of the image is somber and contemplative.'}\n",
      "images/5.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image shows a naked woman standing in a dimly lit room with a stone pillar on the right side. She is facing away from the camera, with her back towards the camera. On the left side of the image, there is a man wearing a black robe and holding a sword. He is standing in front of a candelabra with several lit candles on it. The background is blurred, but it appears to be a medieval-style room with wooden walls and pillars. The overall mood of the scene is dark and eerie.'}\n",
      "images/6.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image shows two women in a room with a bed in the background. The woman on the left is naked and has long curly hair, which is styled in loose curls. She is wearing a necklace and appears to be looking down at the other woman, who is standing in front of her with a serious expression on her face. The other woman is also naked and is holding a black object in her hand. Both women are wearing green dresses and appear to be engaged in a conversation. The room is dimly lit and there is a window on the right side of the image.'}\n",
      "images/7.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': \"The image shows a naked woman lying on her back on a bed, with her eyes closed and her head resting on a pillow. She appears to be in a peaceful and relaxed state, with a slight smile on her face. The background is dark and out of focus, making the woman and the man the focal point of the image. The woman's body is partially covered by the pillow, and her arms are stretched out to the sides. The image is taken from a low angle, with the woman's head slightly tilted to the side. The lighting is soft and warm, creating a dreamy and intimate atmosphere.\"}\n",
      "images/8.jpg\n",
      "{'<MORE_DETAILED_CAPTION>': 'The image is a photograph of a young woman sitting on a rocky beach. She is completely naked, with her body facing the camera and her arms resting on her knees. Her long dark hair is flowing in the wind and cascades over her shoulders. The woman is looking off into the distance with a slight smile on her face. The background shows the ocean and a clear blue sky. The overall mood of the image is peaceful and serene.'}\n"
     ]
    }
   ],
   "source": [
    "# iterate through them all. and describe. \n",
    "for i in range(1, 9):\n",
    "    print(f\"images/{i}.jpg\")\n",
    "    print(generate_detailed_caption(f\"images/{i}.jpg\"))  # False if SFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MORE_DETAILED_CAPTION>': \"The image is a close-up of a person's hand holding a watch. The background is dark and blurred, but it appears to be an outdoor setting with trees and a cloudy sky. The focus of the image is on the hand and the watch, which is partially visible in the foreground. The watch has a gold-colored face and a black strap. The image is taken from a low angle, making the watch stand out against the dark background.\"}\n"
     ]
    }
   ],
   "source": [
    "image_path = \"images/10.jpg\"  # Replace with your image file path\n",
    "detailed_caption = generate_detailed_caption(image_path)\n",
    "print(detailed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MORE_DETAILED_CAPTION>': 'The image shows a man and a woman in a kitchen. The man is standing in front of a shelf with stacks of toilet paper and other household items on it. He is wearing a brown jacket and has a beard. The woman is sitting on the countertop next to him, wearing a blue tank top and black shorts. They are both looking at each other and appear to be engaged in a conversation. On the right side of the image, there is a sign that reads \"I\\'m a sweet guy.\"'}\n"
     ]
    }
   ],
   "source": [
    "image_path = \"images/13.jpg\"  # Replace with your image file path\n",
    "detailed_caption = generate_detailed_caption(image_path)\n",
    "print(detailed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSMLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
