{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the idea here is that we simply replace all profane words with some word like shit. This includes all verbs and adjectives like fucking bad, or fucked up and it would simply be replaced by shiting bad or shited up. Now obviously this isnt right grammar, so we make a model fix that grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ai This is shit stupid and shit up\n",
      "Original: This is fucking stupid and fucked up\n",
      "Cleaned:  This is incredibly foolish and needs immediate attention.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ollama\n",
    "\n",
    "# List of profane words to replace\n",
    "PROFANITY_DICT = {\n",
    "    r\"\\bfuck(ed|ing)?\\b\": \"shit\",\n",
    "    r\"\\bass(holes?)?\\b\": \"shit\",\n",
    "    r\"\\bbitch(es)?\\b\": \"shit\",\n",
    "    r\"\\bdamn(ed)?\\b\": \"shit\",\n",
    "    r\"\\bhell\\b\": \"shit\",\n",
    "}\n",
    "\n",
    "\n",
    "def replace_profanity(text):\n",
    "    \"\"\"Replace profane words with modified replacements.\"\"\"\n",
    "    for pattern, replacement in PROFANITY_DICT.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def fix_grammar(text):\n",
    "    print(\"before ai\", text)\n",
    "    \"\"\"Use Ollama to fix grammar after profanity replacement.\"\"\"\n",
    "    prompt = f'Only provide the answer. Do not give suggestions. Fix the grammar in this sentence while keeping the meaning unchanged: \"{text}\"'\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    \"\"\"Pipeline to replace profanity and correct grammar.\"\"\"\n",
    "    censored_text = replace_profanity(input_text)\n",
    "    corrected_text = fix_grammar(censored_text)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"This is fucking stupid and fucked up\"\n",
    "cleaned_sentence = clean_text(input_sentence)\n",
    "\n",
    "print(\"Original:\", input_sentence)\n",
    "print(\"Cleaned:\", cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load T5 model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcorrect_grammar_t5\u001b[39m(text):\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1736\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1736\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1724\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1722\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "\n",
    "def correct_grammar_t5(text):\n",
    "    \"\"\"Corrects grammar using a T5 model\"\"\"\n",
    "    input_text = f\"Correct the grammar: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(correct_grammar_t5(\"This is shiting stupid and shited up!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is shiting stupid and shited up !\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def basic_grammar_correction(text):\n",
    "    \"\"\"Simple rule-based correction\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    corrected = []\n",
    "    for word in words:\n",
    "        if word.endswith(\"ing\") and word[:-3] not in wordnet.words():\n",
    "            corrected.append(word[:-3])  # Remove 'ing' if it's incorrect\n",
    "        elif word.endswith(\"ed\") and word[:-2] not in wordnet.words():\n",
    "            corrected.append(word[:-2])  # Remove 'ed' if it's incorrect\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return \" \".join(corrected)\n",
    "\n",
    "\n",
    "print(basic_grammar_correction(\"This is shiting stupid and shited up!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Krishnaraj\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
