{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Krishnaraj\\.cache\\huggingface\\hub\\models--MU-NLPC--whisper-tiny-audio-captioning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.whisper.configuration_whisper.WhisperConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and processor\u001b[39;00m\n\u001b[0;32m      6\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMU-NLPC/whisper-tiny-audio-captioning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Move model to GPU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load and preprocess audio\u001b[39;00m\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[1;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.whisper.configuration_whisper.WhisperConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import transformers\n",
    "from transformers import WhisperForAudioCaptioning\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "checkpoint = \"MU-NLPC/whisper-tiny-audio-captioning\"\n",
    "model = WhisperForAudioCaptioning.from_pretrained(checkpoint).to(\"cuda\")  # Move to GPU if available\n",
    "tokenizer = transformers.WhisperTokenizer.from_pretrained(checkpoint, language=\"en\", task=\"transcribe\")\n",
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(checkpoint)\n",
    "\n",
    "# Load and preprocess audio\n",
    "input_file = \"your_audio_file.wav\"  # Change this to your actual file path\n",
    "audio, sampling_rate = librosa.load(input_file, sr=feature_extractor.sampling_rate)\n",
    "features = feature_extractor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
    "\n",
    "# Prepare caption style (optional but helps with formatting)\n",
    "style_prefix = \"clotho > caption: \"\n",
    "style_prefix_tokens = tokenizer(\n",
    "    \"\", text_target=style_prefix, return_tensors=\"pt\", add_special_tokens=False\n",
    ").labels.to(\"cuda\")\n",
    "\n",
    "# Generate caption\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs=features,\n",
    "        forced_ac_decoder_ids=style_prefix_tokens,\n",
    "        max_length=100,  # Control verbosity\n",
    "    )\n",
    "\n",
    "# Decode and print the caption\n",
    "caption = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Generated Audio Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (5.2.0)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from librosa) (0.4)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting standard-aifc (from librosa)\n",
      "  Downloading standard_aifc-3.13.0-py3-none-any.whl.metadata (969 bytes)\n",
      "Collecting standard-sunau (from librosa)\n",
      "  Downloading standard_sunau-3.13.0-py3-none-any.whl.metadata (914 bytes)\n",
      "Requirement already satisfied: packaging in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n",
      "  Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting standard-chunk (from standard-aifc->librosa)\n",
      "  Downloading standard_chunk-3.13.0-py3-none-any.whl.metadata (860 bytes)\n",
      "Collecting audioop-lts (from standard-aifc->librosa)\n",
      "  Downloading audioop_lts-0.2.1-cp313-abi3-win_amd64.whl.metadata (1.7 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading msgpack-1.1.0-cp313-cp313-win_amd64.whl (75 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "Downloading standard_aifc-3.13.0-py3-none-any.whl (10 kB)\n",
      "Downloading standard_sunau-3.13.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Downloading audioop_lts-0.2.1-cp313-abi3-win_amd64.whl (30 kB)\n",
      "Downloading standard_chunk-3.13.0-py3-none-any.whl (4.9 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: standard-chunk, soxr, pycparser, msgpack, audioread, audioop-lts, standard-sunau, standard-aifc, pooch, cffi, soundfile, librosa\n",
      "Successfully installed audioop-lts-0.2.1 audioread-3.0.1 cffi-1.17.1 librosa-0.11.0 msgpack-1.1.0 pooch-1.8.2 pycparser-2.22 soundfile-0.13.1 soxr-0.5.0.post1 standard-aifc-3.13.0 standard-chunk-3.13.0 standard-sunau-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['', '']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, PreTrainedTokenizerFast\n",
    "import torchaudio\n",
    "\n",
    "audio_file_path = \"audio/rain.mp3\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use the model trained on AudioCaps\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"wsntxxn/effb2-trm-audio-captioning\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"wsntxxn/audiocaps-simple-tokenizer\"\n",
    ")\n",
    "\n",
    "# inference on a single audio clip\n",
    "wav, sr = torchaudio.load(audio_file_path)\n",
    "wav = torchaudio.functional.resample(wav, sr, model.config.sample_rate)\n",
    "if wav.size(0) > 1:\n",
    "    wav = wav.mean(0).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    word_idxs = model(\n",
    "        audio=wav,\n",
    "        audio_length=[wav.size(1)],\n",
    "    )\n",
    "\n",
    "caption = tokenizer.decode(word_idxs[0], skip_special_tokens=True)\n",
    "print(caption)\n",
    "\n",
    "# inference on a batch\n",
    "wav1, sr1 = torchaudio.load(audio_file_path)\n",
    "wav1 = torchaudio.functional.resample(wav1, sr1, model.config.sample_rate)\n",
    "wav1 = wav1.mean(0) if wav1.size(0) > 1 else wav1[0]\n",
    "\n",
    "wav2, sr2 = torchaudio.load(audio_file_path)\n",
    "wav2 = torchaudio.functional.resample(wav2, sr2, model.config.sample_rate)\n",
    "wav2 = wav2.mean(0) if wav2.size(0) > 1 else wav2[0]\n",
    "\n",
    "wav_batch = torch.nn.utils.rnn.pad_sequence([wav1, wav2], batch_first=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    word_idxs = model(\n",
    "        audio=wav_batch,\n",
    "        audio_length=[wav1.size(0), wav2.size(0)],\n",
    "    )\n",
    "\n",
    "captions = tokenizer.batch_decode(word_idxs, skip_special_tokens=True)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from efficientnet_pytorch) (2.6.0+cu126)\n",
      "Requirement already satisfied: filelock in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (pyproject.toml): started\n",
      "  Building wheel for efficientnet_pytorch (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16520 sha256=56530814876d06b182560439afbe0248310b9bc664cef6385d67de4e1ffa138e\n",
      "  Stored in directory: c:\\users\\krishnaraj\\appdata\\local\\pip\\cache\\wheels\\5b\\2f\\2c\\f72934c756bb8333dc80c448b1c97e40665b27b7fd15d6be9f\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSMLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
