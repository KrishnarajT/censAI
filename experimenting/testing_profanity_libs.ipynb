{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: profanity-check in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.2 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from profanity-check) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from scikit-learn>=0.20.2->profanity-check) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from scikit-learn>=0.20.2->profanity-check) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from scikit-learn>=0.20.2->profanity-check) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from scikit-learn>=0.20.2->profanity-check) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install profanity-check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprofanity_check\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m predict, predict_prob\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is some bad profanity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict([text]))  \u001b[38;5;66;03m# 1 means profane, 0 means clean\u001b[39;00m\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\profanity_check\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofanity_check\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m predict, predict_prob\n\u001b[0;32m      2\u001b[0m __version__\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mp:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\profanity_check\\profanity_check.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from profanity_check import predict, predict_prob\n",
    "\n",
    "text = \"This is some bad profanity.\"\n",
    "print(predict([text]))  # 1 means profane, 0 means clean\n",
    "print(predict_prob([text]))  # Probability of profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Programs\\DSML\\DSMLEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'toxicity', 'score': 0.010831213556230068}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"unitary/unbiased-toxic-roberta\")\n",
    "text = \"This is some bad profanity.\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'obscene', 'score': 0.9813066720962524}]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some fucked profanity.\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'obscene', 'score': 0.993635892868042}]\n"
     ]
    }
   ],
   "source": [
    "text = \"fuck\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'toxicity', 'score': 0.011274803429841995}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Its all because of the cocks\"  # some got dialogue\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting profanity-filter\n",
      "  Using cached profanity_filter-1.3.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting cached-property<2.0,>=1.5 (from profanity-filter)\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting more-itertools<9.0,>=8.0 (from profanity-filter)\n",
      "  Using cached more_itertools-8.14.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ordered-set<4.0,>=3.0 (from profanity-filter)\n",
      "  Using cached ordered-set-3.1.1.tar.gz (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of profanity-filter to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting profanity-filter\n",
      "  Using cached profanity_filter-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached profanity_filter-1.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached profanity_filter-1.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached profanity_filter-1.1.14-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting more-itertools<8.0,>=7.0 (from profanity-filter)\n",
      "  Using cached more_itertools-7.2.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting profanity-filter\n",
      "  Using cached profanity_filter-1.1.13-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.12-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.11-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of profanity-filter to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached profanity_filter-1.1.10-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.9-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.8-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.7-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.6-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached profanity_filter-1.1.5-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached profanity_filter-1.1.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached profanity_filter-1.1.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached profanity_filter-1.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached profanity_filter-1.1.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "  Using cached profanity_filter-1.0.16-py3-none-any.whl.metadata (9.1 kB)\n",
      "  Using cached profanity_filter-1.0.15-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Using cached profanity_filter-1.0.14-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Using cached profanity_filter-1.0.13-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Using cached profanity_filter-1.0.12-py3-none-any.whl.metadata (8.0 kB)\n",
      "  Using cached profanity_filter-1.0.10-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Using cached profanity_filter-1.0.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached profanity_filter-1.0.8-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached profanity_filter-1.0.7-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached profanity_filter-1.0.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached profanity_filter-1.0.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached profanity_filter-1.0.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached profanity_filter-1.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached profanity_filter-1.0.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    profanity-filter 1.3.3 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.3.2 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.3.1 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.3.0 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.14 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.13 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.12 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.11 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.10 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.9 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.8 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.7 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.6 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.5 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.4 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.3 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.2 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.1 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.1.0 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.16 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.15 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.14 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.13 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.12 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.10 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.9 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.8 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.7 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.6 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.5 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.4 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.3 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "    profanity-filter 1.0.2 depends on ordered-set-stubs<0.2.0 and >=0.1.3\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install profanity-filter==1.0.10, profanity-filter==1.0.12, profanity-filter==1.0.13, profanity-filter==1.0.14, profanity-filter==1.0.15, profanity-filter==1.0.16, profanity-filter==1.0.2, profanity-filter==1.0.3, profanity-filter==1.0.4, profanity-filter==1.0.5, profanity-filter==1.0.6, profanity-filter==1.0.7, profanity-filter==1.0.8, profanity-filter==1.0.9, profanity-filter==1.1.0, profanity-filter==1.1.1, profanity-filter==1.1.10, profanity-filter==1.1.11, profanity-filter==1.1.12, profanity-filter==1.1.13, profanity-filter==1.1.14, profanity-filter==1.1.2, profanity-filter==1.1.3, profanity-filter==1.1.4, profanity-filter==1.1.5, profanity-filter==1.1.6, profanity-filter==1.1.7, profanity-filter==1.1.8, profanity-filter==1.1.9, profanity-filter==1.3.0, profanity-filter==1.3.1, profanity-filter==1.3.2 and profanity-filter==1.3.3 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "# lets try other ones\n",
    "%pip install profanity-filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'profanity_filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprofanity_filter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfanityFilter\n\u001b[0;32m      3\u001b[0m pf \u001b[38;5;241m=\u001b[39m ProfanityFilter()\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is some bad profanity.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'profanity_filter'"
     ]
    }
   ],
   "source": [
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "pf = ProfanityFilter()\n",
    "text = \"This is some bad profanity.\"\n",
    "print(pf.is_profane(text))  # Returns True if profane\n",
    "print(pf.censor(text))  # Replaces profanity with ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: better-profanity in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install better-profanity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "This is some bad profanity.\n"
     ]
    }
   ],
   "source": [
    "from better_profanity import profanity\n",
    "\n",
    "text = \"This is some bad profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "This is some **** profanity.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some fucked profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "This is some **** profanity.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some bullshit profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Its all because of the ****.\n"
     ]
    }
   ],
   "source": [
    "text = \"Its all because of the cocks.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Krishnaraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "bad_words = {\"badword1\", \"badword2\"}  # Add your own list\n",
    "text = \"This is some badword1 in a sentence.\"\n",
    "\n",
    "words = word_tokenize(text.lower())\n",
    "contains_profanity = any(word in bad_words for word in words)\n",
    "\n",
    "print(contains_profanity)  # True if profane words found\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some fucked word in a sentence.\"\n",
    "\n",
    "words = word_tokenize(text.lower())\n",
    "contains_profanity = any(word in bad_words for word in words)\n",
    "print(contains_profanity)  # True if profane words found\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so we got 2 working methods here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(text):\n",
    "    print(profanity.contains_profanity(text))  # True if profane\n",
    "    print(profanity.censor(text))  # Censors profane words\n",
    "\n",
    "\n",
    "def ai(text):\n",
    "    result = classifier(text)\n",
    "    print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(text):\n",
    "    print(\"normal\")\n",
    "    normal(text)\n",
    "    print(\"ai\")\n",
    "    ai(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "False\n",
      "hello\n",
      "ai\n",
      "[{'label': 'toxicity', 'score': 0.0007360868039540946}]\n"
     ]
    }
   ],
   "source": [
    "compare(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "True\n",
      "This is some ****\n",
      "ai\n",
      "[{'label': 'obscene', 'score': 0.9857422113418579}]\n"
     ]
    }
   ],
   "source": [
    "compare(\"This is some bullshit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "True\n",
      "This is some place the bulls have been using to ****\n",
      "ai\n",
      "[{'label': 'obscene', 'score': 0.977712869644165}]\n"
     ]
    }
   ],
   "source": [
    "compare(\"This is some place the bulls have been using to shit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "True\n",
      "its all because of the ****\n",
      "ai\n",
      "[{'label': 'toxicity', 'score': 0.018069908022880554}]\n"
     ]
    }
   ],
   "source": [
    "compare(\"its all because of the cocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "False\n",
      " men doing all the fighting\n",
      "ai\n",
      "[{'label': 'male', 'score': 0.9968990087509155}]\n"
     ]
    }
   ],
   "source": [
    "compare(\" men doing all the fighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: You piece of shit! | Profanity Score: 0.0011\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def detect_profanity(text):\n",
    "    \"\"\"Detects whether the given text contains profanity.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return scores[0][1].item()  # Probability of being profane\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "texts = [\n",
    "    \"You piece of shit!\",  # English\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    score = detect_profanity(text)\n",
    "    print(f\"Text: {text} | Profanity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(text):\n",
    "    print(\"normal\")\n",
    "    normal(text)\n",
    "    print(\"ai\")\n",
    "    print(detect_profanity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "True\n",
      "****\n",
      "ai\n",
      "0.0020136770326644182\n"
     ]
    }
   ],
   "source": [
    "compare(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetext in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: lingua-language-detector==1.4.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from safetext) (1.4.0)\n",
      "Requirement already satisfied: pysrt in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from safetext) (1.1.2)\n",
      "Requirement already satisfied: requests in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from safetext) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from safetext) (1.0.1)\n",
      "Requirement already satisfied: brotli<2.0.0,>=1.1.0 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from lingua-language-detector==1.4.0->safetext) (1.1.0)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.9.11 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from lingua-language-detector==1.4.0->safetext) (2024.11.6)\n",
      "Requirement already satisfied: chardet in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from pysrt->safetext) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests->safetext) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests->safetext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests->safetext) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in p:\\programs\\dsml\\dsmlenv\\lib\\site-packages (from requests->safetext) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install safetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Whitelist file not found for language 'en'. Using an empty whitelist.\n"
     ]
    }
   ],
   "source": [
    "from safetext import SafeText\n",
    "\n",
    "st = SafeText(language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'cocks', 'index': 10, 'start': 44, 'end': 49},\n",
       " {'word': 'dicks', 'index': 12, 'start': 54, 'end': 59},\n",
       " {'word': 'fucking', 'index': 5, 'start': 17, 'end': 24}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = st.check_profanity(\n",
    "    text=\"Some text with a fucking word. Men are just cocks and dicks\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'ass', 'index': 2, 'start': 6, 'end': 9}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = st.check_profanity(\n",
    "    text=\"asset ass\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some text with a ******* word. Men are just ***** and *****'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = st.censor_profanity(\n",
    "    text=\"Some text with a fucking word. Men are just cocks and dicks\"\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change profane sentence to sound natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load OpenAI GPT model for paraphrasing\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "\n",
    "def smart_censor(text):\n",
    "    \"\"\"Replaces profane text with a grammatically correct alternative.\"\"\"\n",
    "    prompt = f\"Paraphrase this without profanity: {text}\"\n",
    "    result = paraphraser(prompt, max_length=50, truncation=True)\n",
    "    return result[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase this without profanity: This is fucking ridiculous!\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "text = \"This is fucking ridiculous!\"\n",
    "print(smart_censor(text))  # Example Output: \"This is absolutely ridiculous!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "#     prompt = f\"Dont give any other response. Only give one response. Remove all profanity from this text while keeping the meaning intact.: '{text}'\" works well\n",
    "\n",
    "# also works fine Dont give any other response. Only give one response. Remove all profanity from the given text while keeping the meaning intact. If its just one word, replace with a more friendly word. Do not give advice. If it contains some sexual implication, try to translate to more kid friendly words. Here is the given text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    prompt = f\"\"\"Return only the sanitized text. Remove all profanity while preserving the original meaning. If a single word is profane, replace it with a more appropriate alternative. For text with sexual implications, rewrite it in a kid-friendly manner. Do not provide explanations or advice. Only output the modified text. Give only 1 sentence\n",
    "    \n",
    "    Here are some examples of how to sanitize text:  \n",
    "\n",
    "    Input: \"Fuck you!\"  \n",
    "    Output: \"Screw you!\"  \n",
    "\n",
    "    Input: \"This is so fucked!\"  \n",
    "    Output: \"This is a total mess!\"  \n",
    "\n",
    "    Input: \"Men think with their dicks?\"  \n",
    "    Output: \"Men often prioritize materialistic things?\"  \n",
    "\n",
    "    Input: \"She was sniffing my ass!\"  \n",
    "    Output: \"She was very close to me last night!\"  \n",
    "\n",
    "    Input: \"I wanna fuck you so badly\"  \n",
    "    Output: \"I want us to be close.\" \n",
    "    \n",
    "    Input: \"Have you never boned a woman's ass before jon snow?\"  \n",
    "    Output: \"Have you never been close to a woman before, Jon Snow?\" \n",
    "    \n",
    "\n",
    "    Now, sanitize the following text while preserving its meaning: \n",
    "    \n",
    "    : '{text}'\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Men often prioritize physical possessions.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"men think with their dicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You are extremely unintelligent and disrespectful.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"You are a fucking idiotic whore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I want to be close with you.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"i wanna fuck you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' She was very close to me last night.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"She was sniffing my ass last night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Blimey\"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"Doesn\\'t using something regularly cause discomfort if it\\'s not used?\"'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"Dont your stones start to hurt if you dont use it? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Have you never been close to a woman before, Jon Snow?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"Have you never boned a woman's ass before jon snow? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Screw you!\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"Fuck you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSMLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
