{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install profanity-check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profanity_check import predict, predict_prob\n",
    "\n",
    "text = \"This is some bad profanity.\"\n",
    "print(predict([text]))  # 1 means profane, 0 means clean\n",
    "print(predict_prob([text]))  # Probability of profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"unitary/unbiased-toxic-roberta\")\n",
    "text = \"This is some bad profanity.\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some fucked profanity.\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"fuck\"\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Its all because of the cocks\"  # some got dialogue\n",
    "result = classifier(text)\n",
    "print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try other ones\n",
    "%pip install profanity-filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profanity_filter import ProfanityFilter\n",
    "\n",
    "pf = ProfanityFilter()\n",
    "text = \"This is some bad profanity.\"\n",
    "print(pf.is_profane(text))  # Returns True if profane\n",
    "print(pf.censor(text))  # Replaces profanity with ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install better-profanity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "\n",
    "text = \"This is some bad profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some fucked profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some bullshit profanity.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Its all because of the cocks.\"\n",
    "print(profanity.contains_profanity(text))  # True if profane\n",
    "print(profanity.censor(text))  # Censors profane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "bad_words = {\"badword1\", \"badword2\"}  # Add your own list\n",
    "text = \"This is some badword1 in a sentence.\"\n",
    "\n",
    "words = word_tokenize(text.lower())\n",
    "contains_profanity = any(word in bad_words for word in words)\n",
    "\n",
    "print(contains_profanity)  # True if profane words found\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some fucked word in a sentence.\"\n",
    "\n",
    "words = word_tokenize(text.lower())\n",
    "contains_profanity = any(word in bad_words for word in words)\n",
    "print(contains_profanity)  # True if profane words found\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# so we got 2 working methods here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(text):\n",
    "    print(profanity.contains_profanity(text))  # True if profane\n",
    "    print(profanity.censor(text))  # Censors profane words\n",
    "\n",
    "\n",
    "def ai(text):\n",
    "    result = classifier(text)\n",
    "    print(result)  # Gives toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(text):\n",
    "    print(\"normal\")\n",
    "    normal(text)\n",
    "    print(\"ai\")\n",
    "    ai(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"This is some bullshit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"This is some place the bulls have been using to shit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"its all because of the cocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\" men doing all the fighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def detect_profanity(text):\n",
    "    \"\"\"Detects whether the given text contains profanity.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return scores[0][1].item()  # Probability of being profane\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "texts = [\n",
    "    \"You piece of shit!\",  # English\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    score = detect_profanity(text)\n",
    "    print(f\"Text: {text} | Profanity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(text):\n",
    "    print(\"normal\")\n",
    "    normal(text)\n",
    "    print(\"ai\")\n",
    "    print(detect_profanity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install safetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetext import SafeText\n",
    "\n",
    "st = SafeText(language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = st.check_profanity(\n",
    "    text=\"Some text with a fucking word. Men are just cocks and dicks\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = st.censor_profanity(\n",
    "    text=\"Some text with a fucking word. Men are just cocks and dicks\"\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change profane sentence to sound natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load OpenAI GPT model for paraphrasing\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "\n",
    "def smart_censor(text):\n",
    "    \"\"\"Replaces profane text with a grammatically correct alternative.\"\"\"\n",
    "    prompt = f\"Paraphrase this without profanity: {text}\"\n",
    "    result = paraphraser(prompt, max_length=50, truncation=True)\n",
    "    return result[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "text = \"This is fucking ridiculous!\"\n",
    "print(smart_censor(text))  # Example Output: \"This is absolutely ridiculous!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "#     prompt = f\"Dont give any other response. Only give one response. Remove all profanity from this text while keeping the meaning intact.: '{text}'\" works well\n",
    "\n",
    "# also works fine Dont give any other response. Only give one response. Remove all profanity from the given text while keeping the meaning intact. If its just one word, replace with a more friendly word. Do not give advice. If it contains some sexual implication, try to translate to more kid friendly words. Here is the given text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    prompt = f\"\"\"Return only the sanitized text. Remove all profanity while preserving the original meaning. If a single word is profane, replace it with a more appropriate alternative. For text with sexual implications, rewrite it in a kid-friendly manner. Do not provide explanations or advice. Only output the modified text. Give only 1 sentence\n",
    "    \n",
    "    Here are some examples of how to sanitize text:  \n",
    "\n",
    "    Input: \"Fuck you!\"  \n",
    "    Output: \"Screw you!\"  \n",
    "\n",
    "    Input: \"This is so fucked!\"  \n",
    "    Output: \"This is a total mess!\"  \n",
    "\n",
    "    Input: \"Men think with their dicks?\"  \n",
    "    Output: \"Men often prioritize materialistic things?\"  \n",
    "\n",
    "    Input: \"She was sniffing my ass!\"  \n",
    "    Output: \"She was very close to me last night!\"  \n",
    "\n",
    "    Input: \"I wanna fuck you so badly\"  \n",
    "    Output: \"I want us to be close.\" \n",
    "    \n",
    "    Input: \"Have you never boned a woman's ass before jon snow?\"  \n",
    "    Output: \"Have you never been close to a woman before, Jon Snow?\" \n",
    "    \n",
    "\n",
    "    Now, sanitize the following text while preserving its meaning: \n",
    "    \n",
    "    : '{text}'\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"men think with their dicks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"You are a fucking idiotic whore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"i wanna fuck you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"She was sniffing my ass last night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"Dont your stones start to hurt if you dont use it? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"Have you never boned a woman's ass before jon snow? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"Fuck you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
